{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMEN : scrapping de l'ensemble des produits du site OPENFOODFACTS\n",
    "\n",
    "https://fr.openfoodfacts.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Présenté par : \n",
    "| Prénoms       |     Nom         |   \n",
    "| ------------- |: -------------: |\n",
    "| Amadou lamarana      | DIALLO               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce livrable constitue un véritable exercice sur l'extraction des données avec le langage Python. Nous avons pour ce faire utilisé les libraries essentielles que sont:\n",
    "* **Pandas**: libraire pour la gestion et la manipulation des données **cvs** récupérées\n",
    "* **BeautifulSoup**:librairie pour l'extraction des fichiers HTML et XML\n",
    "* **re**: Librairie des expressions regulières\n",
    "* **requests**: pour l'interrogation des **url**  (requêtes HTTP)\n",
    "* **unicodedata**: Module de conversion de caractères spéciaux au format **utf-8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À travers ce notebook , nous allons extraire les liens des produits qui constituent les données de bases\n",
    "du présent projet.\n",
    "Un petit aperçu du site montre que le nombre de page est assez grand plus **7980 pages**<br/>\n",
    "Nous allons récuperer en suivant les étapes citées ci-après:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### <div class='text text-primary'>Procédures de récupération des données </div>\n",
    "1. Récupérer les urls qui pointe sur toutes ces pages\n",
    "    * Définition des fonctions pour récupérer les pages du site\n",
    "2. Récuperer les urls des produits contenu dans chacune des pages\n",
    "    * Définition de la fonction qui recupère les urls de produits d'une page\n",
    "    * Sauvegarder les liens dans un fichier au format **csv**\n",
    "3. Récupération des informations des produits\n",
    "4. Création du Dataframe et l'export sous CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='text text-primary' style='background:green; color:white; font-size:16px; padding:26px'> 1. Récupérer les liens des pages </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des modules nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "## Opendata\n",
    "## import des modules nécessaires\n",
    "import pandas as pd \n",
    "import unicodedata\n",
    "from os import path\n",
    "pd.set_option(\"max_colwidth\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://fr.openfoodfacts.org\" ## endpoind du site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRODUCT_URL_FILE ='url_product.csv' ## fichier des liens des produits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE_URL_FILE ='url_page.csv' ## fichier des liens des pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE ='OFF_DATA.csv' ## fichier contenant les données extraites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_FILE ='OFF_STATUS.csv' ## fichier état et avancement du scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='text text-primary' style='background:#0350F0; color:white; font-size:15px; padding:20px'>Fonction qui récupere les pages</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nous allons procéder par deux méthodes**\n",
    "\n",
    "    1. Par lecture directe et génération des liens\n",
    "    2. Par lecture d'un fichier de sauvegarde des liens des pages\n",
    "    \n",
    "Nous effectuerons des mesures de performaces et de rapide pour choisir la meilleure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='text' style='background:#000000; color:white; font-size:15px; padding:20px'>1 ere méthode : Lecture directe sur le site à chaque exécution</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recuperer_urls_page():\n",
    "    \"\"\"\n",
    "        * Cette fonction retourne la liste des pages du site \n",
    "        * Cette liste initialisé avec l'url de valeur BASE_URL\n",
    "        * Une requête html avec ** request.get(BASE_URL)** pour tenter de récuperer le contenu de l'accueil\n",
    "        * Si la page est accessible, une série de manipulation avec BeautifoulSoup sera déclenchée pour \n",
    "        * suivant le format de liens, BASE_URL/n avec n le numéro de page, nous allons itérer \n",
    "            en tenant compte du numéro de la dernière page disponible sur le site.\n",
    "    @return list: urls\n",
    "    \"\"\"\n",
    "    #Initiate urls \n",
    "    urls= [BASE_URL] ## url de base initialisé avec le lien d'accueil du site (lien de base)\n",
    "    accueil = requests.get(BASE_URL) ## request html \n",
    "    if accueil.status_code == requests.codes.ok: ## All is OK (codeResponse = 200)\n",
    "        ## parser html\n",
    "        bs = BeautifulSoup(accueil.text, 'html.parser') \n",
    "        ## la pagination est accessible par la balise 'ul' et par l'identifiant \"pages\"\n",
    "        ulPages = bs.find('ul',id='pages')\n",
    "        ## récupérer les éléments de la pagination avec la balise 'li'\n",
    "        page = ulPages.findAll('li', class_='')\n",
    "        ## extrait avec le module :'re' des numéros de pages qui seront stockés dans une liste \"numeros\"\n",
    "        numeros = [ int(n.text) for n in page if n.text in re.findall('[0-9]*', n.text)]\n",
    "        derniere_page = max(numeros)\n",
    "        for i in range(2,derniere_page+1):\n",
    "            url = BASE_URL+'/'+str(i)\n",
    "            urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='text text-primary' style='background:#FF50F0; color:white; font-size:15px; padding:20px'>Temps d\"éxecution: 2.46 s </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 109 ms, sys: 25.5 ms, total: 134 ms\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "methode_1_urls_page = recuperer_urls_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='text' style='background:#000000; color:white; font-size:15px; padding:20px'>2 ieme méthode: Lecture des liens sur un fichier de sauvegarde</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recuperer_lien_page():\n",
    "    \n",
    "    \"\"\"\n",
    "        * Cette fonction retourne la liste des pages du site \n",
    "        * Cette liste initialisé avec l'url de valeur BASE_URL\n",
    "        * Une requête html avec ** request.get(BASE_URL)** pour tenter de récuperer le contenu de l'accueil\n",
    "        * Si la page est accessible, une série de manipulation avec BeautifoulSoup sera déclenchée pour \n",
    "        * suivant le format de liens, BASE_URL/n avec n le numéro de page, nous allons itérer \n",
    "            en tenant compte du numéro de la dernière page disponible sur le site.\n",
    "        @return list: urls\n",
    "    \"\"\"\n",
    "    accueil = requests.get(BASE_URL) ## request html \n",
    "    \n",
    "    if accueil.status_code == requests.codes.ok: ## All is OK (codeResponse = 200)\n",
    "        ## parser html\n",
    "        \n",
    "        bs = BeautifulSoup(accueil.text, 'html.parser') \n",
    "        \n",
    "        ## la pagination est accessible par la balise 'ul' et par l'identifiant \"pages\"\n",
    "        \n",
    "        pagination = bs.find('ul',id='pages')\n",
    "        \n",
    "        ## récupérer les éléments de la pagination avec la balise 'li'\n",
    "        \n",
    "        pages = pagination.findAll('li', class_='')\n",
    "        \n",
    "        ## extrait avec le module :'re' des numéros de pages qui seront stockés dans une liste \"numeros\"\n",
    "        \n",
    "        numeros = [ int(n.text) for n in pages if n.text in re.findall('[0-9]*', n.text)]\n",
    "        \n",
    "        ## Vérifier s'il y déja un fichier des urls produits en csv existant\n",
    "        if path.exists(PAGE_URL_FILE):\n",
    "            try:\n",
    "                ## urls nouveaux à concatener aux urls existants dans le fichier \n",
    "                nouveaux_urls= []\n",
    "                ## le fichier existe et n'est pas vide\n",
    "                DF = pd.read_csv(PAGE_URL_FILE, engine='python')\n",
    "                ## on recupère le nombre de page enregistrées dans le fichier csv\n",
    "                nombre_page  = DF.shape[0]\n",
    "                ## nombre de page traité\n",
    "                nombre_page_non_traite=  max(numeros) -nombre_page\n",
    "                \n",
    "                if nombre_page_non_traite >0:\n",
    "                    \n",
    "                    ## s'il y a de nouvelles pages . On effectue une boucle de la page suivante du dernier url du\n",
    "                    ## du dataset PAGE_URL_FILE jusqu'a la fin\n",
    "                    \n",
    "                    for i in range(nombre_page+1, max(numeros)+1):\n",
    "                        \n",
    "                        ## on génère le lien // aka: BASE_URL + \"/\"+i\n",
    "                        \n",
    "                        nouveau_url = BASE_URL+'/'+str(i)\n",
    "                        \n",
    "                        ## on ajoute à la liste\n",
    "                        \n",
    "                        nouveaux_urls.append(nouveau_url)\n",
    "                        \n",
    "                    ## en fin de boucle nous avons les nouveaux liens depuis la dernière mise à jour \n",
    "                    ## des liens des pages\n",
    "                    \n",
    "                    ## on effectue l'ajout des nouveaux urls sur la liste déja disponble\n",
    "                    nouveau_DF = pd.DataFrame(nouveaux_urls)\n",
    "                    nouveau_DF.columns=['url_page']\n",
    "                    \n",
    "                    #list_urls_disponible = list(DF['url_page'])\n",
    "                    \n",
    "                    ## concaténation \n",
    "                    #list_page_urls = list_page_urls + nouveaux_urls\n",
    "                    newDF = pd.concat([DF, nouveau_DF], ignore_index=True)\n",
    "                    \n",
    "                    ## on génére un nouveau fichier csv pour une sauvegarde et base d'une prochaine vérification\n",
    "                    #newDF = pd.DataFrame(list_page_urls)\n",
    "                    #newDF.columns =['url_page']\n",
    "                    newDF.to_csv(PAGE_URL_FILE, index=False , encoding='utf-8')\n",
    "                    \n",
    "            except pd.errors.EmptyDataError:\n",
    "                print('File empty')\n",
    "        else:\n",
    "            #Initialiser la liste des liens \n",
    "    \n",
    "            urls= [BASE_URL] \n",
    "            ## effectuer une boucle de la deuxieme page jusqu'a la fin\n",
    "            \n",
    "            for i in range(2, max(numeros)+1):\n",
    "                ## ajouter le numero de la page sur le lien de la page\n",
    "                url = BASE_URL+'/'+str(i)\n",
    "                ## ajouter le lien de la page sur la liste des urls\n",
    "                urls.append(url)\n",
    "                \n",
    "            ## une  fois la boucle terminée , nous avons une liste de toutes les pages .. à savugarder \n",
    "            DF = pd.DataFrame(urls)\n",
    "            \n",
    "            ## on attribue un nom à la colonne\n",
    "            DF.columns = ['url_page']\n",
    "            \n",
    "            ## on sauvegarde le dataFrame en fichier csv pour une prochaine utilisation\n",
    "            DF.to_csv(PAGE_URL_FILE, index=False, encoding = 'utf-8')\n",
    "            \n",
    "    ## en appelant cette fonction on renvoie la DataFrame du fichier stocké (\"url_page.csv\")\n",
    "    return pd.read_csv(PAGE_URL_FILE, engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='text text-primary' style='background:#635FF0; color:white; font-size:15px; padding:20px'>Génerer le dataframe des pages</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous générons notre DataFrame pandas de la liste de pages du site ..  et nous notons une légère amélioration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='text text-primary' style='background:#09FF90; color:white; font-size:19px;font-weight:bold; padding:20px'>Temps d\"éxecution: 2.14 s </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 13.4 ms, total: 165 ms\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DF_Page_url = recuperer_lien_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://fr.openfoodfacts.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://fr.openfoodfacts.org/2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://fr.openfoodfacts.org/3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://fr.openfoodfacts.org/4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://fr.openfoodfacts.org/5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8073</th>\n",
       "      <td>https://fr.openfoodfacts.org/8074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8074</th>\n",
       "      <td>https://fr.openfoodfacts.org/8075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8075</th>\n",
       "      <td>https://fr.openfoodfacts.org/8076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076</th>\n",
       "      <td>https://fr.openfoodfacts.org/8077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8077</th>\n",
       "      <td>https://fr.openfoodfacts.org/8078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8078 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               url_page\n",
       "0          https://fr.openfoodfacts.org\n",
       "1        https://fr.openfoodfacts.org/2\n",
       "2        https://fr.openfoodfacts.org/3\n",
       "3        https://fr.openfoodfacts.org/4\n",
       "4        https://fr.openfoodfacts.org/5\n",
       "...                                 ...\n",
       "8073  https://fr.openfoodfacts.org/8074\n",
       "8074  https://fr.openfoodfacts.org/8075\n",
       "8075  https://fr.openfoodfacts.org/8076\n",
       "8076  https://fr.openfoodfacts.org/8077\n",
       "8077  https://fr.openfoodfacts.org/8078\n",
       "\n",
       "[8078 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_Page_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class='text text-primary' style='background:#0350F0; color:white; font-size:15px; padding:20px'>Étape 2: Récuperer les liens des produits</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comme la méthode précedente, nous procéderons d'abord par un chargement à partir d'une extraction sur le site et ensuite la méthode de lecture à partir d'un fichier et effectuer des modifications si notées sur le site**\n",
    "\n",
    "\n",
    "On définit deux fonctions :\n",
    "\n",
    "    + recuperer_url_produits_page(url_page): retourne les liens des produits dans une page\n",
    "    \n",
    "    + recuperer_urls_produits_site(urls_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='text' style='background:#000000; color:white; font-size:15px; padding:20px'>1 iere méthode: Lecture et chargement suite extraction sur le site</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recuperer_url_produits_page(url_page):\n",
    "    \"\"\"Fonction qui renvoie les urls des produits listé dans la page du lien passé en paramètre\"\"\"\n",
    "    page = requests.get(url_page)\n",
    "    if page.status_code == requests.codes.ok:\n",
    "        bs = BeautifulSoup(page.text, 'html.parser')\n",
    "        bloc_produits = bs.find('ul', class_ ='products')\n",
    "        #print(bs)\n",
    "        urls_produits =[]\n",
    "        les_produits = bloc_produits.findAll('li',class_='')\n",
    "        if les_produits is not None and les_produits != []:\n",
    "            urls_produits = [BASE_URL+url.find(\"a\")['href'] for url in les_produits if url is not None]\n",
    "        return urls_produits\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #####  ETAPE 2. 2  Recupérer les urls de tous des produits de toute les pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recuperer_urls_produits_site(urls_pages):\n",
    "    \"\"\"Focntion qui récupére les liens des produits du site\n",
    "        * Nous utilisons la fonction \"recuperer_urls_page\" pour avoir les liens des pages;\n",
    "        * Nous utilisons la fonction \"recuperer_url_produits_page\"  pour récupérer \n",
    "        les liens des produits par page\n",
    "        @return list: urls_produits\n",
    "    \"\"\"\n",
    "    urls_produits =[]\n",
    "    i = 0\n",
    "    ## les produits par page\n",
    "    for page in urls_pages:\n",
    "        i= i+1\n",
    "        if i%100 ==0:\n",
    "            print(\"Page :\",i)\n",
    "        #time.sleep(random.uniform(0.2,0.5))\n",
    "        lien_produits = recuperer_url_produits_page(page)\n",
    "        if lien_produits is not None and lien_produits !=[]:\n",
    "            urls_produits = urls_produits + lien_produits\n",
    "    return urls_produits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='text text-primary' style='background:#FF0000; color:white; font-size:15px; padding:20px'>Temps d\"éxecution: xx minutes </div>\n",
    "\n",
    "Nous n'allons pas exécuter cette requête chaque fois que cela est nécessaire .. Nous allons optimiser en definissant une fonction qui utilise le résultat d'une première exécution (qui est indispensable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## recuperer les urls des produits, enregistrer à la variable urls_produits\n",
    "urls_produits_site = recuperer_urls_produits_site(methode_1_urls_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class='text text-primary' style='background:#FF0000; color:white; font-size:15px; padding:20px'>Cette méthode prends bcp de temps à récuperer les données de liens des produits. Plus de 5h pour extraire juste les liens des produits</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class='text text-primary' style='background:#FF0000; color:white; font-size:15px; padding:20px'>Une optimisation est nécessaire pour l'extraction ds données du site en entier .<br>\n",
    "D'abord nous allons améliorer en utilisant un fichier de sauvegade. Ensuite Nous utiliserons Scrappy qui est plus rapide et plus efficace et plus performant.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> On va procéder de la même manière que lors de la génération des liens des pages .. \n",
    "> <b>On part d'un fichier des urls des produits existant où nous recupérons le nombre de liens enregistrés et procéder au complément, suivant le nombre de page disponible.</b> <br/>\n",
    "> <b>Sinon si le fichier n'existe pas, il faut le créer </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimiser_recuperer_urls_produit(df_page):\n",
    "    \"\"\"\n",
    "    Cette fonction renvoie les liens des produits à partir du dataFrame des urls des pages en argument\n",
    "    et retourne un dataframe pandas des liens.\n",
    "    \"\"\"\n",
    "    ## Nous partons de l'existance ou non du fichier des liens des produits\n",
    "    ## s'il existe c'est une mise à jour \n",
    "    ## sinon, c'est une création avec les liens de tous les produits.\n",
    "    \n",
    "    ## verifier le type du paramètre // aka il faut un dataframe. sinon KO\n",
    "    if isinstance(df_page, pd.DataFrame):\n",
    "        ## Si mise à jour (si le fichier url_product.csv existe)\n",
    "        if path.exists(PRODUCT_URL_FILE):\n",
    "            try:\n",
    "\n",
    "                ## on lit le fichier csv\n",
    "                DF = pd.read_csv(PRODUCT_URL_FILE, engine='python')\n",
    "                ## on determine le nombre de lien enregistrés dans le fichier\n",
    "                nombre_liens_produits = DF.shape[0] ## 80\n",
    "                \n",
    "                ## nombre de pages \n",
    "                nombre_page = df_page.shape[0] ## 1\n",
    "                \n",
    "                ## determiner le nombre de pages traité et le mobre de lien de la derniere page\n",
    "                nombre_page_traite = nombre_liens_produits//100 ## ->0+1\n",
    "                \n",
    "                ## nombre de liens produits traité de la derniere page\n",
    "                ## le reste de divion par 100 \n",
    "                nombre_liens_produit_derniere_page = nombre_liens_produits%100\n",
    "                \n",
    "                ## on verifie s'il y a de mettre à jour .. avec obligation de vérfier le la derniere page s'il y\n",
    "                ## ajout de nouveau produit. aka. le nombre de page peut ne pas changé et que le nombre de produit de la \n",
    "                ## derniere page change\n",
    "                \n",
    "                if nombre_page_traite == nombre_page:\n",
    "                    print('Derniere déja page traitée:\\n')\n",
    "                    print('Vérification de changement sur la derniere page..:\\n')\n",
    "                    ## Mise à jour des urls de la derniere page. \n",
    "                    ## juste le nombre de produit de la dernière page aurait peut-être changé \n",
    "                    page = requests.get(df_page.iloc[-1,:]['url_page'])\n",
    "                    \n",
    "                    ## si la page est accessible\n",
    "                    if page.status_code == requests.codes.ok:\n",
    "                        ## parser le resultat en html\n",
    "                        bs = BeautifulSoup(page.text, 'html.parser')\n",
    "                        ## intexer le bloc de la balise 'ul' ayant la classe 'products' \n",
    "                        bloc_produits = bs.find('ul',class_='products')\n",
    "                        ## liste des produits de la page\n",
    "                        les_produits = bloc_produits.findAll('li',class_='')\n",
    "                        ## verifier si le nombre a changé, (c'est dire s'il de nouveaux produits dans la page)\n",
    "                        \n",
    "                        if nombre_liens_produit_derniere_page:\n",
    "                            ## le nombre de produits de la page a changé\n",
    "                            \n",
    "                            ## recupération des nouveaux urls et sauvegarder dans une liste nouveaux_urls_produits\n",
    "                            ## faire une boucle des sur les produits et récupérer les liens \n",
    "                            nouveaux_urls_produits = [BASE_URL+url.find(\"a\")['href'] for url in les_produits[nombre_liens_produit_derniere_page+1:]]\n",
    "                            \n",
    "                            ##creation d'un dataframe\n",
    "                            newdf_urls_produits = pd.DataFrame(nouveaux_urls_produits)\n",
    "                            newdf_urls_produits.columns = ['url_product']\n",
    "                            \n",
    "                            ## concatenation avec le dataframe originale\n",
    "                            print(\"Création du dataframe..\\n\")\n",
    "                            DF = pd.concat([DF, newdf_urls_produits], ignore_index=True)\n",
    "                            \n",
    "                            ## sauvegarder la derniere version \n",
    "                            DF.to_csv(PRODUCT_URL_FILE, index=False, encoding = 'utf-8')\n",
    "                            print(\"Changement sur la derniere page integrée..\\n\")\n",
    "                            ## retourner le dataFrame\n",
    "                            return DF\n",
    "                        else:\n",
    "                            ## si pas de changement sur la derniere page .. on renvoie le DF non changé\n",
    "                            print(\"Pas de changement sur le site fr.openfoodfact.org .......\\n\")\n",
    "                            print(\"On retourne le dataframe URL des produits........\\n\")\n",
    "                            return DF\n",
    "                elif nombre_page_traite  < nombre_page:\n",
    "                    ## si le nombre de page  du dataframe des urls pages est supérieure\n",
    "                    print(\"Le nombre de page récupérée est supérieure au au nombre de page antérieure..\")\n",
    "                    print(\"Une mise à jour oblige... \\n\")\n",
    "                    n_rows = nombre_page_traite *100\n",
    "                    \n",
    "                    ##  on reduit le nombre de ligne .. et ignore les dernieres\n",
    "                    DF = DF.iloc[0:n_rows,:]\n",
    "                    \n",
    "                    ##recupérer les liens de page suivante jusqu'a la fin\n",
    "                    page_restante = list(df_page.iloc[nombre_page_traite+1:,:]['url_page'])\n",
    "                    \n",
    "                    nouveaux_urls_produits =[] ## liste pour sauvegarder les liens nouveaux\n",
    "                    for url_page in page_restante:\n",
    "                        \n",
    "                        page = requests.get(url_page)\n",
    "                        if page.status_code == requests.codes.ok:\n",
    "                            \n",
    "                            bs = BeautifulSoup(page.text, 'html.parser')\n",
    "                            bloc_produits = bs.find('ul',class_='products')\n",
    "                            ## liste des produits de la page\n",
    "                            les_produits = bloc_produits.findAll('li',class_='')\n",
    "                           \n",
    "                            nouveaux_urls_produits = nouveaux_urls_produits +[BASE_URL+url.find(\"a\")['href'] for url in les_produits]\n",
    "                            \n",
    "                    ##creation d'un dataframe\n",
    "                    newdf_urls_produits = pd.DataFrame(nouveaux_urls_produits)\n",
    "                    newdf_urls_produits.columns = ['url_product']\n",
    "                    ## concatenation avec le dataframe originale\n",
    "                    DF = pd.concat([DF, newdf_urls_produits], ignore_index=True)\n",
    "                            \n",
    "                    ## sauvegarder la derniere version \n",
    "                    DF.to_csv(PRODUCT_URL_FILE, index=False, encoding = 'utf-8')\n",
    "                    #print(DF)\n",
    "                    #newDF = \n",
    "                    ## faire une boucle sur les pages non traité et récuperer les urls des produits.\n",
    "                    \n",
    "                    return DF\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(\"Impossible d'ouvrir le fichier\")\n",
    "                \n",
    "        else:\n",
    "            ## le fichier n'existe pas .. donc on le crée sous format csv\n",
    "            print(\"Le fichier csv n'existe pas .. nous allons le créer \\n\")\n",
    "            nouveaux_urls_produits =[]\n",
    "            for url_page in list(df_page['url_page']):\n",
    "                page = requests.get(url_page)\n",
    "                if page.status_code == requests.codes.ok:\n",
    "                    bs = BeautifulSoup(page.text, 'html.parser')\n",
    "                    bloc_produits = bs.find('ul',class_='products')\n",
    "                    ## liste des produits de la page\n",
    "                    les_produits = bloc_produits.findAll('li',class_='')\n",
    "\n",
    "                    nouveaux_urls_produits = nouveaux_urls_produits +[BASE_URL+url.find(\"a\")['href'] for url in les_produits]\n",
    "            \n",
    "            print(\"Création du dataframe......... \\n\")\n",
    "            DF =  pd.DataFrame(nouveaux_urls_produits)\n",
    "            print(\"Sauveagder du dataframe......... \\n\")\n",
    "            DF.to_csv(PRODUCT_URL_FILE, index=False, encoding = 'utf-8')\n",
    "            \n",
    "            return DF\n",
    "    else:\n",
    "        ## l'objet n'est pas un dataframe.. \n",
    "        print(\"Ce n'est pas un DataFrame\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de page récupérée est supérieure au au nombre de page antérieure..\n",
      "Une mise à jour oblige... \n",
      "\n",
      "CPU times: user 9.25 s, sys: 1.74 s, total: 11 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Df_Url_produit = optimiser_recuperer_urls_produit(DF_Page_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='text text-primary' style='background:#FF0000; color:white; font-size:15px; padding:20px'>Nous avons un dataframe de liens de produits de 807200 lignes</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "807640"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Df_Url_produit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls_produits_site = pd.read_csv('url_product.csv')\n",
    "urls_produits = list(df_urls_produits_site['url_product'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Etape 3 : Fonction de récupération des informations des produits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### On part du fichier csv des url des produits..\n",
    "\n",
    "df_urls_produits_site = pd.read_csv('url_product.csv')\n",
    "urls_produits = list(df_urls_produits_site['url_product'])\n",
    "\n",
    "# ####  Etape 3 : Recupération des informations des produits \n",
    "len(urls_produits)\n",
    "\n",
    "\n",
    "def supprimer_espaces_carcteres_speciaux(text):\n",
    "    \"\"\"Supprime les retours à la ligne et normalise le texte, \n",
    "    remplace les tabulations, l'apostrphe \"\"\"\n",
    "    if text is not None:\n",
    "        text = text.replace('\\t','').replace(\"'\",'_').replace('\\n','').replace('\"','_').strip().lower()\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "    else:\n",
    "        return 'XXX'\n",
    "    #return unicodedata.normalize(\"NFKD\", text.replace('\\n',' ').replace('\\t',' ').replace(\"'\",'_').strip().lower())\n",
    "    \n",
    "def convertible(text):\n",
    "    chiffres = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "    pattern = re.compile(r'[+-]?([0-9]*[.])?[0-9]+$')\n",
    "    if text=='XXX':\n",
    "        return 'XXX'\n",
    "    elif (text[0] not in chiffres) and len(text)>1:\n",
    "        #print(text[1:])\n",
    "        if (text[0] == '-') and (\"%\" in text):\n",
    "            return 'XXX'\n",
    "        else:\n",
    "            text = text[1:].replace(',','.').strip()\n",
    "            test = re.match(pattern, text)\n",
    "            if test:\n",
    "                return float(text)\n",
    "            else:\n",
    "                return 'XXX'\n",
    "    elif text[0] in chiffres:\n",
    "        try:\n",
    "            test = re.match(pattern, text)\n",
    "            if test:\n",
    "                return float(text)\n",
    "            else:\n",
    "                return 'XXX'\n",
    "        except:\n",
    "            return 'XXX'\n",
    "    else:\n",
    "        return \"XXX\"\n",
    "\n",
    "## function qui renvoie le nom\n",
    "def recuperer_nom(product_tag):\n",
    "    \"\"\"Renvoie le nom s'il existe sinon XXX\"\"\"\n",
    "    nom_produit='XXX'\n",
    "    if product_tag:\n",
    "        nom_brut = product_tag.find('h1',itemprop='name')\n",
    "        if nom_brut:\n",
    "            nom_produit = nom_brut.get_text(strip=True).strip()\n",
    "    return nom_produit\n",
    "\n",
    "def recuperer_code_barre(product):\n",
    "    \"\"\"Renvoie le code barre du produit\"\"\"\n",
    "    code_barre='XXX'\n",
    "    if product:\n",
    "        code_barre_tag = product.find('span',id=\"barcode\", itemprop=\"gtin13\")\n",
    "        if code_barre_tag:\n",
    "            code_barre = code_barre_tag.text.strip()\n",
    "\n",
    "    return code_barre\n",
    "\n",
    "def recuperer_nutriscore(product_tag):\n",
    "    \"\"\"Renvoie le nutri-score\"\"\"\n",
    "    nutri_score ='XXX'\n",
    "    if product_tag:\n",
    "        score_brut = product_tag.find('div',id='nutriscore_drop') \n",
    "        if score_brut:\n",
    "            nutri_score = supprimer_espaces_carcteres_speciaux(score_brut.find_all('p')[-1].text.split(':')[-1]).upper()\n",
    "        \n",
    "    return nutri_score\n",
    "\n",
    "def recuperer_nova(product_tag):\n",
    "    \"\"\"retour le nova score\"\"\"\n",
    "    ## nova\n",
    "    nova ='XXX'\n",
    "    if product_tag:\n",
    "        nova_nb =  product_tag.find_all('a',href='/nova')\n",
    "        if len(nova_nb) > 1:\n",
    "            nova_nb =nova_nb[1]\n",
    "            valeur = nova_nb.find('img').attrs['alt'].split(' - ')[0]\n",
    "            try:\n",
    "                pattern = re.compile(r'[+-]?([0-9]*[.])?[0-9]+$')\n",
    "                test = re.match(pattern, valeur)\n",
    "                if test:\n",
    "                    nova = int(valeur) if valeur.isdigit() else 'XXX'\n",
    "                else:\n",
    "                    nova ='XXX'\n",
    "            except:\n",
    "                nova ='XXX'\n",
    "    return nova\n",
    "\n",
    "def recuperer_eco_score(product_tag):\n",
    "    \"\"\"Retour l'eco_score\"\"\"\n",
    "    ## score \n",
    "    eco_score ='XXX'\n",
    "    if product_tag:\n",
    "        b_eco_score_nb =  product_tag.find_all('a',href='/ecoscore')\n",
    "        if b_eco_score_nb:\n",
    "            eco_score_img = b_eco_score_nb[-1].find('img')\n",
    "            if eco_score_img is not None:\n",
    "                eco_score = eco_score_img.attrs['alt'].split()[-1]\n",
    "    \n",
    "    return eco_score\n",
    "\n",
    "def recuperer_caracteristique(all_p_tag):\n",
    "    \"\"\"Retour la caracteristique du produit\"\"\"\n",
    "    caracteristique='XXX'\n",
    "    if all_p_tag is not None:\n",
    "        for p in all_p_tag:\n",
    "            if \"Dénomination générique\" in p.text:\n",
    "                caracteristique = supprimer_espaces_carcteres_speciaux(p.text.split(':')[1])\n",
    "                break    \n",
    "    return caracteristique\n",
    "\n",
    "def recuperer_quantite(all_p_tag):\n",
    "    \"\"\"Retour la quantité du produit\"\"\"\n",
    "    quantite='XXX'\n",
    "    if all_p_tag is not None:\n",
    "        for p in all_p_tag:\n",
    "            if \"Quantité\" in p.text:\n",
    "                quantite = supprimer_espaces_carcteres_speciaux(p.text.split(':')[1])\n",
    "                break \n",
    "    return quantite\n",
    "def recuperer_conditionnement(all_p_tag):\n",
    "    \"\"\"Retourne le conditionnement du produit\"\"\"\n",
    "    conditionnement =\"XXX\"\n",
    "    if all_p_tag is not None:\n",
    "        for p in all_p_tag:\n",
    "            if 'Conditionnement' in p.text:\n",
    "                conditionnement =supprimer_espaces_carcteres_speciaux(p.text.split(':')[1])\n",
    "                break\n",
    "    return conditionnement\n",
    "\n",
    "def recuperer_marques(all_p_tag):\n",
    "    \"\"\"Retourne le conditionnement du produit\"\"\"\n",
    "    marques =\"XXX\"\n",
    "    if all_p_tag is not None:\n",
    "        for p in all_p_tag:\n",
    "            if 'Marques' in p.text:\n",
    "                marques =supprimer_espaces_carcteres_speciaux(p.text.split(':')[1])\n",
    "                break\n",
    "    return marques\n",
    "def recuperer_categories(all_p_tag):\n",
    "    \"\"\"Retourne l.a.es catégorie.s\"\"\"\n",
    "    categories ='XXX'\n",
    "    if all_p_tag is not None:\n",
    "        for p in all_p_tag:\n",
    "            if \"Catégories\" in p.text:\n",
    "                categories = supprimer_espaces_carcteres_speciaux(p.text.split(':')[1])\n",
    "                break\n",
    "    return categories\n",
    "def recuperer_lieu_fabrication(all_p_tag):\n",
    "    \"\"\"Recuperer le lieu de fabrication\"\"\"\n",
    "    lieu_fabrication ='XXX'\n",
    "    if all_p_tag is not None:\n",
    "        for p in all_p_tag:\n",
    "            if \"Lieux de fabrication ou de transformation\" in p.text:\n",
    "                lieu_fabrication = supprimer_espaces_carcteres_speciaux(p.text.split(':')[1].strip())\n",
    "                break\n",
    "    return lieu_fabrication\n",
    "                                                     \n",
    "def recuperer_code_tracabilite(all_p_tag):\n",
    "    code_tracabilite ='XXX'\n",
    "    if all_p_tag is not None:\n",
    "        for p in all_p_tag:\n",
    "            if \"Code de traçabilité\" in p.text:\n",
    "                code_tracabilite = supprimer_espaces_carcteres_speciaux(p.text.split(':')[1].strip())\n",
    "                break\n",
    "    return code_tracabilite\n",
    "\n",
    "def recuperer_lien_produit_page_site_fabricant(all_p_tag):\n",
    "    \"\"\"Recupere le lien du produit sur le site du fabriquant\"\"\"\n",
    "    lien_produit_page_site_fabricant='XXX'\n",
    "    if all_p_tag is not None:\n",
    "        for p in all_p_tag:\n",
    "            if \"Lien vers la page du produit sur le site officiel du fabricant\" in p.text:\n",
    "                lien_produit_site_fabricant_0 = p.find('a').attrs['href'] if p.find('a') is not None else 'XXX'\n",
    "                lien_produit_page_site_fabricant = supprimer_espaces_carcteres_speciaux(lien_produit_site_fabricant_0)\n",
    "                break\n",
    "    return lien_produit_page_site_fabricant\n",
    "def recuperer_magasins(all_p_tag):\n",
    "    magasins ='XXX'\n",
    "    if all_p_tag is not None:\n",
    "        for p in all_p_tag:\n",
    "            if \"Magasins\" in p.text:\n",
    "                magasins = p.text.split(':')[1]\n",
    "                break\n",
    "    return magasins\n",
    "\n",
    "def recuperer_label_certif_recompense(all_p_tag):\n",
    "    \"\"\"Recuperer label, certification, recompense\"\"\"\n",
    "    label_certif_recompense ='XXX'\n",
    "    if all_p_tag is not None:\n",
    "        for p in all_p_tag:\n",
    "            if \"Labels, certifications, récompenses\" in p.text:\n",
    "                label_certif_recompense= supprimer_espaces_carcteres_speciaux(p.text.split(':')[1])\n",
    "                break\n",
    "    return label_certif_recompense\n",
    "def recuperer_pays_de_vente(all_p_tag):\n",
    "    pays_de_vente ='XXX'\n",
    "    if all_p_tag is not None:\n",
    "        for p in all_p_tag:\n",
    "            if \"Pays de vente\" in p.text:\n",
    "                pays_de_vente = supprimer_espaces_carcteres_speciaux(p.text.split(':')[1])\n",
    "                break\n",
    "    return pays_de_vente\n",
    "def recuperer_origin_ingredient(ingredient_tag):\n",
    "    \"\"\"Recuperer l'origine des ingrédients\"\"\"\n",
    "    origin ='XXX'\n",
    "    if ingredient_tag:\n",
    "        origin_brut= ingredient_tag.find('div', id='ecoscore_panel_origins').find('ul')\n",
    "        if origin_brut:\n",
    "            origin =supprimer_espaces_carcteres_speciaux(origin_brut.text.replace('\\n',','))\n",
    "    return origin\n",
    "\n",
    "def recuperer_analyse(analyse_tag):\n",
    "    \"\"\"Recuperer les analyses\"\"\"\n",
    "    analyse ='XXX'\n",
    "    if analyse_tag:\n",
    "        analyse_brut = analyse_tag.find('p',id='ingredients_analysis_ingredients_text')\n",
    "        if analyse_brut:\n",
    "            analyse = supprimer_espaces_carcteres_speciaux(analyse_brut.text)\n",
    "    return analyse\n",
    "def recuperer_additifs(additif_tag):\n",
    "    \"\"\"Recuperer les additifs du produits\"\"\"\n",
    "    additifs = 'XXX'\n",
    "    if additif_tag:\n",
    "        additifs_brut = additif_tag[0].find('ul') \n",
    "        if additifs_brut:\n",
    "            additifs = supprimer_espaces_carcteres_speciaux(additifs_brut.text)\n",
    "    return additifs\n",
    "\n",
    "def recuperer_huile_de_palme(huile_palme_tag):\n",
    "    \"\"\"Recuperer huile de palme features\"\"\"\n",
    "    huile_palme='XXX'\n",
    "    if huile_palme_tag:\n",
    "        huile_palme_brut = huile_palme_tag[1].find('ul')\n",
    "        if huile_palme_brut:\n",
    "            huile_palme = supprimer_espaces_carcteres_speciaux(huile_palme_brut.text)\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    return huile_palme\n",
    "def recuperer_matiere_grasse(matiere_grasse_tag):\n",
    "    \"\"\"Recuperer la matiere grasse du produit\"\"\"\n",
    "    matiere_grasse ='XXX'\n",
    "    if matiere_grasse_tag:\n",
    "        matiere_grasse_text = matiere_grasse_tag.find('td', class_='nutriment_value')\n",
    "        if matiere_grasse_text:\n",
    "            matiere_grasse =matiere_grasse_text.text.strip().split()[0].replace(',','.')\n",
    "            matiere_grasse =convertible(matiere_grasse) if (matiere_grasse_text !='XXX') and (matiere_grasse[0] not in['?']) else 'XXX' ### 4.9 g\n",
    "    #print(matiere_grasse)\n",
    "    return matiere_grasse\n",
    "\n",
    "def recuperer_acide_gras(acide_gras_tag):\n",
    "    \"\"\"Recuperer l'acide gras \"\"\"\n",
    "    Acide_Gras ='XXX'\n",
    "    if acide_gras_tag:\n",
    "        acide_gras_brut = acide_gras_tag.find('td',class_='nutriment_value')\n",
    "        if acide_gras_brut:\n",
    "            acide_gras_text = supprimer_espaces_carcteres_speciaux(acide_gras_brut.text) \n",
    "            Acide_Gras = convertible(acide_gras_text.split()[0].replace(',','.'))\n",
    "    #print(\"Acide Gras:\",Acide_Gras)\n",
    "    return Acide_Gras\n",
    "def recuperer_sucre(sucre_tag):\n",
    "    \"\"\"Recupere la quantité de sucrre\"\"\"\n",
    "    sucre ='XXX'\n",
    "    if sucre_tag:\n",
    "        sucre_text = sucre_tag.find('td', class_='nutriment_value')\n",
    "        if sucre_text:\n",
    "            sucre = convertible(sucre_text.text.strip().split()[0].replace(',','.'))\n",
    "    #print(sucre)\n",
    "    return sucre\n",
    "def recuperer_sel(sel_tag):\n",
    "    \"\"\"Recupere la quantité de sel\"\"\"\n",
    "    sel='XXX'\n",
    "    if sel_tag:\n",
    "        sel_text = sel_tag.find('td', class_='nutriment_value')\n",
    "        if sel_text:\n",
    "            sel = convertible(sel_text.text.strip().split()[0].replace(',','.'))\n",
    "    return sel\n",
    "\n",
    "def recuperer_comparaison(comparaison_tag):\n",
    "    \"\"\"Retourne la catégorie de comparaison\"\"\"\n",
    "    comparaison ='XXX'\n",
    "    if comparaison_tag:\n",
    "        comparaison = supprimer_espaces_carcteres_speciaux(comparaison_tag.text)\n",
    "    return comparaison\n",
    "\n",
    "def recuperer_energie_kj(energie_kj_tag):\n",
    "    \"\"\"Retourne l'enrgie en kj\"\"\"\n",
    "    energie_kj='XXX'\n",
    "    if energie_kj_tag:\n",
    "        energie_kj_text =energie_kj_tag.find('td',class_='nutriment_value')\n",
    "        if energie_kj_text:\n",
    "            energie_kj =convertible(energie_kj_text.text.strip().split('kj')[0].replace(\",\",'').replace(\" \",\"\").replace(u'\\xa0',''))\n",
    "    return energie_kj\n",
    "\n",
    "def recuperer_energie_kcal(energie_kcal_tag):\n",
    "    \"\"\"retourne l'energie en kcal\"\"\"\n",
    "    energie_kcal ='XXX'\n",
    "    if energie_kcal_tag:\n",
    "        energie_kcal_text =energie_kcal_tag.find('td',class_='nutriment_value')\n",
    "        if energie_kcal_text:\n",
    "            energie_kcal =convertible(energie_kcal_text.text.strip().split('kcal')[0].replace(\",\",'').replace(\" \",\"\").replace(u'\\xa0',''))\n",
    "    \n",
    "    return energie_kcal\n",
    "\n",
    "def recuperer_les_allgergies(allergie_tag):\n",
    "    #print(allergie_tag)\n",
    "    allergies=''\n",
    "    if allergie_tag and allergie_tag is not None:\n",
    "        for alg_tag in allergie_tag:\n",
    "            if allergies =='':\n",
    "                allergies = alg_tag.text.lower()\n",
    "            else:\n",
    "                allergies = allergies +','+ alg_tag.text.lower()\n",
    "    else:\n",
    "        allergies= 'NaN'\n",
    "    return allergies\n",
    "\n",
    "def get_data(url_product):\n",
    "    \"\"\" Recupération des informations essentielles du produit du lien en paramètre \n",
    "    @url_product : string\n",
    "    @return data: list\n",
    "    \"\"\"\n",
    "    ## recuperer la reponse http\n",
    "    page =  requests.get(url_product)\n",
    "   \n",
    "    ## transformer au format html\n",
    "    bs = BeautifulSoup(page.text, 'lxml')\n",
    "    \n",
    "    ## detecter la balise div avec itemtype non vide\n",
    "    product = bs.find(\"div\",itemtype =re.compile(r\"\\w\"))\n",
    "    \n",
    "    ##Nom du produit\n",
    "    \n",
    "    nom_produit= recuperer_nom(product)\n",
    "    ## code barre\n",
    "\n",
    "    code_barre = recuperer_code_barre(product)\n",
    "   \n",
    "    ## nutriscore\n",
    "    nutri_score =  recuperer_nutriscore(product)\n",
    "    \n",
    "    ## nova\n",
    "    nova = recuperer_nova(product)\n",
    "    \n",
    "    ## eco score\n",
    "    eco_score = recuperer_eco_score(product)\n",
    "    \n",
    "    ## les balises p sans classe successifs et dont la reconnaissance se fait par le texte \n",
    "    ## i.e Quantité: pour la valeur quantite\n",
    "    all_p = None\n",
    "    if product is not None :\n",
    "        div_paragraph = product.find('div', class_='medium-12 large-8 xlarge-8 xxlarge-8 columns')\n",
    "        if div_paragraph is not None:\n",
    "            all_p = div_paragraph.find_all('p')\n",
    "    ## caracteristique\n",
    "    caracteristique =recuperer_caracteristique(all_p)\n",
    "    ## quantite\n",
    "    quantite = recuperer_quantite(all_p)\n",
    "    ## conditionnement\n",
    "    conditionnement = recuperer_conditionnement(all_p)\n",
    "    ## marques\n",
    "    marques = recuperer_marques(all_p)\n",
    "    ## categories\n",
    "    categories = recuperer_categories(all_p)\n",
    "    ## label, certifications, recompenses\n",
    "    label_certif_recompense =recuperer_label_certif_recompense(all_p)\n",
    "    ## lieu de fabrication\n",
    "    lieu_fabrication = recuperer_lieu_fabrication(all_p)\n",
    "    ## code de tracabilité\n",
    "    code_tracabilite =recuperer_code_tracabilite(all_p)\n",
    "    ## lien url du produit sur le site du fabricant \n",
    "    lien_produit_page_site_fabricant = recuperer_lien_produit_page_site_fabricant(all_p)\n",
    "    ## les magasins vente du produit\n",
    "    magasins = recuperer_magasins(all_p)\n",
    "    ## les pays où est vendu le produit\n",
    "    pays_de_vente = recuperer_pays_de_vente(all_p)\n",
    "    ## Origine des ingredients\n",
    "    ingredient_tag = bs.find('div',id='ecoscore_drop')\n",
    "    origin = recuperer_origin_ingredient(ingredient_tag)\n",
    "    ## l'analyse des ingredients\n",
    "    analyse_tag = bs.find(\"div\", id='ingredient_analysis_drop')\n",
    "    analyse = recuperer_analyse(analyse_tag)\n",
    "    ## les additifs\n",
    "    additifs_tag = bs.find_all('div', class_='medium-6 columns')\n",
    "    additifs= recuperer_additifs(additifs_tag)\n",
    "    ## si existance d'huile de palme\n",
    "    huile_palme_tag = bs.find_all('div', class_='medium-6 columns')\n",
    "    huile_palme= recuperer_huile_de_palme(huile_palme_tag)\n",
    "    ## quantité de matiere grasse\n",
    "    matiere_grasse_tag = bs.find('tr', id ='nutriment_fat_tr')\n",
    "    Matiere_grasse = recuperer_matiere_grasse(matiere_grasse_tag)\n",
    "    ## acide gras\n",
    "    acide_gras_tag = bs.find('tr',id='nutriment_saturated-fat_tr')\n",
    "    Acide_Gras = recuperer_acide_gras(acide_gras_tag)\n",
    "    ## sucre \n",
    "    sucre_tag = bs.find('tr',id ='nutriment_sugars_tr')\n",
    "    Sucre = recuperer_sucre(sucre_tag)\n",
    "    ## sel\n",
    "    sel_tag = bs.find('tr',id='nutriment_salt_tr')\n",
    "    Sel = recuperer_sel(sel_tag)\n",
    "    ## catégories de comparaisons\n",
    "    comparaison_tag = bs.find('label', style='display:inline;font-size:1rem;')\n",
    "    comparaison = recuperer_comparaison(comparaison_tag)\n",
    "    ## energie en kj\n",
    "    energie_kj_tag = bs.find('tr',id='nutriment_energy_tr')\n",
    "    energie_kj = recuperer_energie_kj(energie_kj_tag)\n",
    "    ## energie en kcal\n",
    "    energie_kcal_tag = bs.find('tr', id='nutriment_energy-kcal_tr')\n",
    "    nbre_kcal = recuperer_energie_kcal(energie_kcal_tag)\n",
    "    \n",
    "    ## glutten et allergiogene\n",
    "    tag_well_knows = bs.find_all(\"a\",attrs={'class':'tag well_known'})\n",
    "    allergie_tag =[allergie for allergie in tag_well_knows if ('allergene' in allergie.attrs['href'])] \n",
    "    allergies = recuperer_les_allgergies(allergie_tag)\n",
    "    ## les données sont mises en liste et renvoyées\n",
    "    data = [\n",
    "        nom_produit, code_barre, nutri_score, nova, eco_score,caracteristique, \n",
    "        quantite, conditionnement,marques, categories, label_certif_recompense,\n",
    "        lieu_fabrication,code_tracabilite, lien_produit_page_site_fabricant,magasins,pays_de_vente,origin,analyse,additifs,\n",
    "        huile_palme,Matiere_grasse,Acide_Gras,Sucre,Sel,comparaison,\n",
    "        nbre_kcal,energie_kj, allergies\n",
    "    ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_produits.index(\"https://fr.openfoodfacts.org/produit/3274080005003/cristaline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div class='text text-primary' style='background:#0350F0; color:white; font-size:15px; padding:20px'>Étape 4 Récuperation des données et export en CSV</div>\n",
    "<hr>\n",
    "> Dans cette partie, nous allons extraire les données et les stocker dans un fichier au format csv , au fur et à mesure que l'extraction avance. \n",
    "\n",
    "> <div class='text text-primary' style='background:#000000; color:white; font-size:15px; padding:20px'> <b>À TOUT MOMENT ON PEUT INTERROMPRE LE PROGRAMME ET LE REPRENDRE SANS PERTE DE DONNÉES NI DEVOIR RECOMMENCER </b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "off_data = [] ## list des nos données extraites\n",
    "i=0 ## compteur d'enregistrement\n",
    "\n",
    "## les colonnes du DF\n",
    "cols = ['nom_produit', 'code_bar', 'nutri_score', 'nova', 'eco_score','caracteristique', \n",
    "        'quantite', 'conditionnement','marques', 'categories', 'label_certif_recompense',\n",
    "        'lieu_fabrication','code_tracabilite', 'lien_produit_page_site_fabricant','magasin','pays_de_vente','origin','analyse','additifs',\n",
    "        'huile_palme','matiere_grasse','acide_gras','sucre','sel','comparaison',\n",
    "        'nbre_kcal','energie_kcal','allergies']\n",
    "\n",
    "## Verifier si nous avons un fichier qui sauvegarde l'état davancement du sauvegadre des données\n",
    "\n",
    "print(\"Debut de scraping..\")\n",
    "\n",
    "## colonne de du fichier STATUS_FILE\n",
    "col_status = ['last_url']\n",
    "\n",
    "## statut de lecture du dataset csv : par défaut à False\n",
    "readFile = False\n",
    "## index est utlisé pour recevoir l'index du lien du dernier produit récupérer afin de rebondir au suivant lien\n",
    "index = -1\n",
    "if path.exists(STATUS_FILE):\n",
    "    ## lire le fichier status\n",
    "    df_status = pd.read_csv(STATUS_FILE)\n",
    "    ## recuperer le dernier lien traité\n",
    "    last_url = list(df_status['last_url'])[0]\n",
    "    ## recuperer l'index dans les urls des produits\n",
    "    try:\n",
    "        index = urls_produits.index(last_url)\n",
    "    except ValueError:\n",
    "        index =-1\n",
    "\n",
    "## A partir de l'index de l'url du fichier status . la boucle for debutera à l'index suivant \n",
    "## sinon .. on debut à l'index 0\n",
    "if index != -1:\n",
    "    ## concatenation de Dataframe\n",
    "    readFile = True\n",
    "    i = index\n",
    "    \n",
    "if readFile:\n",
    "    for url in urls_produits[index+1:]:\n",
    "        info_product = get_data(url)\n",
    "        off_data.append(info_product)\n",
    "        i=i+1\n",
    "        #time.sleep(0.01)\n",
    "        if i%100 ==0:\n",
    "            print(\"Nombre d'execution i=\",i)\n",
    "            ## onn lit le fichier sauvegarder\n",
    "            DF = pd.read_csv('OFF_DATA_.csv', engine='python')\n",
    "            ## df avec les 100 derniers enregsistrements\n",
    "            df = pd.DataFrame(off_data[-100:],columns=cols)\n",
    "            #df.to_csv(DATA_FILE, encoding='utf-8', index=False)\n",
    "            ## on stocke les données dans le fichier au format csv\n",
    "            ## si données deja enregistrées alors on concatene avec le nouveau dataframe df\n",
    "\n",
    "            newDF = pd.concat([DF, df], ignore_index=True)\n",
    "            newDF.drop_duplicates(ignore_index=True, inplace=True)\n",
    "            newDF.to_csv('OFF_DATA_.csv', encoding='utf-8', index=False)\n",
    "            ## on stocke le dernier lien traité dans un fichier pour une prochaine relance (à programmer)\n",
    "            df_status = pd.DataFrame([url],columns = col_status)\n",
    "\n",
    "            df_status.to_csv(STATUS_FILE,encoding='utf-8', index=False)\n",
    "        \n",
    "else:\n",
    "    for url in urls_produits:\n",
    "        info_product = get_data(url)\n",
    "        off_data.append(info_product)\n",
    "        i=i+1\n",
    "        #time.sleep(0.01)\n",
    "        if i%100 ==0:\n",
    "            print(\"Nombre d'execution i=\",i)\n",
    "            df = pd.DataFrame(off_data,columns=cols)\n",
    "            #df.to_csv(DATA_FILE, encoding='utf-8', index=False)\n",
    "            ## on stocke les données dans le fichier au format csv\n",
    "\n",
    "            ## si données deja enregistrées alors on concatene avec le nouveau dataframe df\n",
    "\n",
    "            df.to_csv('OFF_DATA_.csv', encoding='utf-8', index=False)\n",
    "            ## on stocke le dernier lien traité dans un fichier pour une prochaine relance (à programmer)\n",
    "            df_status = pd.DataFrame([url],columns = col_status)\n",
    "\n",
    "            df_status.to_csv(STATUS_FILE,encoding='utf-8', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made with love.. Amadou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
